{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "colab_type": "code",
    "id": "5YQ7bYMnwJRa",
    "outputId": "65617db9-e466-4ec7-d8e9-d829a89dc2a1"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88zLHqvDwJRj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "import logging\n",
    "import random\n",
    "import itertools\n",
    "from utils import go_utils, rl_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfY-_1_5wJR5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ju4HYnKwJRq"
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DX04C6TbwJR5"
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 256\n",
    "EPISODES_PER_ITERATION = 128\n",
    "MAX_STEPS = 2 * BOARD_SIZE**2\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DX04C6TbwJR5"
   },
   "outputs": [],
   "source": [
    "ACTOR_LEARNING_RATE = 5e-4\n",
    "ACTOR_BETA_1 = 0.9\n",
    "CRITIC_LEARNING_RATE = 1e-3\n",
    "CRITIC_BETA_1 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_DIR = 'model_baselines/'\n",
    "LEVEL_PATHS = [None] + [BASELINE_DIR + filename for filename in ['novice.h5']]\n",
    "STARTING_OPPONENT_PATH = LEVEL_PATHS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EVERY_ITERATION = True\n",
    "LOAD_SAVED_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyALIMpwJRo"
   },
   "source": [
    "# Go Environment\n",
    "Train on a small board with heuristic reward for fast training and efficient debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4w7gMrfwJRp"
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE, reward_method='real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7ZoRfzIwJRr"
   },
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_PATH = 'tmp/actor.h5'\n",
    "CRITIC_PATH = 'tmp/critic.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9wWb0HvwJRs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_actor_critic(mode):\n",
    "    inputs = layers.Input(shape=(BOARD_SIZE, BOARD_SIZE, 4), name=\"board\")\n",
    "    valid_inputs = layers.Input(shape=(BOARD_SIZE**2 + 1,), name=\"valid_moves\")\n",
    "    invalid_values = layers.Input(shape=(BOARD_SIZE**2 + 1,), name=\"invalid_values\")\n",
    "    \n",
    "    x = layers.Flatten()(inputs)\n",
    "    \n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    if mode == 'actor':\n",
    "        move_probs = layers.Dense(128)(x)\n",
    "        move_probs = layers.BatchNormalization()(move_probs)\n",
    "        move_probs = layers.ReLU()(move_probs)\n",
    "        move_probs = layers.Dense(50)(move_probs)\n",
    "        move_probs = layers.Add()([move_probs, invalid_values])\n",
    "        move_probs = layers.Softmax(name=\"move_probs\")(move_probs)\n",
    "        out = move_probs\n",
    "    else:\n",
    "        move_vals = layers.Dense(128)(x)\n",
    "        move_vals = layers.BatchNormalization()(move_vals)\n",
    "        move_vals = layers.ReLU()(move_vals)\n",
    "        move_vals = layers.Dense(50, activation=\"sigmoid\")(move_vals)\n",
    "        move_vals = layers.Multiply(name=\"move_vals\")([move_vals, valid_inputs])\n",
    "        out = move_vals\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, valid_inputs, invalid_values], \n",
    "                           outputs=out, name=mode)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor = make_actor_critic('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = make_actor_critic('critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent = tf.keras.models.clone_model(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_SAVED_MODELS:\n",
    "    actor.load_weights(ACTOR_PATH)\n",
    "    critic.load_weights(CRITIC_PATH)\n",
    "    opponent.load_weights(STARTING_OPPONENT_PATH)\n",
    "    logging.info(\"Loaded models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9djBWO1wJR1"
   },
   "source": [
    "### Initialization of models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98jmZoKvwJRv"
   },
   "outputs": [],
   "source": [
    "mem = []\n",
    "state = go_env.reset()\n",
    "first_action = (2,5)\n",
    "first_action_1d = go_utils.action_2d_to_1d(first_action, BOARD_SIZE)\n",
    "second_action = (5,2)\n",
    "first_state, reward, done, info = go_env.step(first_action)\n",
    "second_state, reward, done, info = go_env.step(second_action)\n",
    "rl_utils.add_to_replay_mem(mem, state, first_action_1d, second_state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.show(rl_utils.sample_heatmaps(actor, critic, mem, num_samples=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViUxxPnUwJR5"
   },
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc1lqGjPwJRm"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for metric_key in ['val_loss', 'exploit_loss', 'explore_loss', 'win_rate', 'num_steps', 'explore_weight']:\n",
    "    metrics[metric_key] = tf.keras.metrics.Mean('{}'.format(metric_key), dtype=tf.float32)\n",
    "metrics['pred_win_acc'] = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writers = {}\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "for summary_key in ['train', 'test']:\n",
    "    log_dir = 'logs/actor_critic/{}/{}'.format(current_time, summary_key)\n",
    "    summary_writers[summary_key] = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXOIVFjmwJR7"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE_WEIGHT = 1e-1\n",
    "EXPLORE_DECAY = 0.9\n",
    "MIN_EXPLORE = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_opt = tf.keras.optimizers.Adam(ACTOR_LEARNING_RATE, ACTOR_BETA_1)\n",
    "def update_actor(replay_mem, iteration):\n",
    "    \"\"\"\n",
    "    Optimizes the actor over the whole replay memory\n",
    "    \"\"\" \n",
    "    batch_indices = np.arange(max(len(replay_mem) // BATCH_SIZE, 1))\n",
    "    np.random.shuffle(batch_indices)\n",
    "    \n",
    "    for batch_idx in tqdm_notebook(batch_indices, desc='Updating actor', leave=False):\n",
    "        states, actions, next_states, winners, terminals = rl_utils.get_batch_obs(replay_mem, BATCH_SIZE, \n",
    "                                                                                  index=batch_idx)\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        # Actor\n",
    "        state_val_distrs = rl_utils.forward_pass(states, critic, training=False)\n",
    "        valid_moves = go_utils.get_valid_moves(states)\n",
    "        with tf.GradientTape() as tape:    \n",
    "            move_prob_distrs = rl_utils.forward_pass(states, actor, training=True)\n",
    "            assert move_prob_distrs.shape == state_val_distrs.shape\n",
    "            win_loss = -tf.math.log(tf.reduce_sum(state_val_distrs * move_prob_distrs, axis=1) + \n",
    "                                    tf.keras.backend.epsilon())\n",
    "            explore_loss = binary_cross_entropy(valid_moves, move_prob_distrs)\n",
    "            explore_weight = max(EXPLORE_WEIGHT * (EXPLORE_DECAY**iteration), MIN_EXPLORE)\n",
    "            move_loss = win_loss + explore_weight * explore_loss\n",
    "        \n",
    "        metrics['exploit_loss'].update_state(win_loss)\n",
    "        metrics['explore_loss'].update_state(explore_loss)\n",
    "        metrics['explore_weight'].update_state(explore_weight)\n",
    "        \n",
    "        # compute and apply gradients\n",
    "        gradients = tape.gradient(move_loss, actor.trainable_variables)\n",
    "        actor_opt.apply_gradients(zip(gradients, actor.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_opt = tf.keras.optimizers.Adam(CRITIC_LEARNING_RATE, CRITIC_BETA_1)\n",
    "def update_critic(replay_mem, iteration):\n",
    "    \"\"\"\n",
    "    Optimizes the critic over the whole replay memory\n",
    "    \"\"\"    \n",
    "    batch_indices = np.arange(max(len(replay_mem) // BATCH_SIZE, 1))\n",
    "    np.random.shuffle(batch_indices)\n",
    "    \n",
    "    for batch_idx in tqdm_notebook(batch_indices, desc='Updating critic', leave=False):\n",
    "        states, actions, next_states, winners, terminals = rl_utils.get_batch_obs(replay_mem, BATCH_SIZE, index=batch_idx)\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        # Critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            move_val_distrs = rl_utils.forward_pass(states, critic, training=True)\n",
    "            move_vals = rl_utils.get_values_for_actions(move_val_distrs, actions)\n",
    "            assert winners.shape == move_vals.shape\n",
    "            val_loss = binary_cross_entropy(winners, move_vals)\n",
    "        \n",
    "        metrics['val_loss'].update_state(val_loss)\n",
    "        metrics['pred_win_acc'].update_state(winners, move_vals > 0.5)\n",
    "        \n",
    "        # compute and apply gradients\n",
    "        gradients = tape.gradient(val_loss, critic.trainable_variables)\n",
    "        critic_opt.apply_gradients(zip(gradients, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNBj_gKPwJR_"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LT1PUXyXwJR_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replay_mem = []\n",
    "recent_ratings = collections.deque(maxlen=1)\n",
    "\n",
    "for iteration in tqdm_notebook(range(ITERATIONS), desc='Iteration'):\n",
    "    # Update other models if appropriate\n",
    "    if SAVE_EVERY_ITERATION:\n",
    "        logging.debug(\"Saving weights...\")\n",
    "        actor.save_weights(ACTOR_PATH)\n",
    "        critic.save_weights(CRITIC_PATH)\n",
    "        \n",
    "    # Opponent update\n",
    "    # Triggered when last few win ratings are all at least a certain threshold\n",
    "    if len(recent_ratings) == recent_ratings.maxlen and reduce(lambda result, rating: result and rating >= 0.75, \n",
    "                                                               recent_ratings, True):\n",
    "        logging.debug(\"Saving weights...\")\n",
    "        actor.save_weights(ACTOR_PATH)\n",
    "        critic.save_weights(CRITIC_PATH)\n",
    "        opponent.load_weights(ACTOR_PATH)\n",
    "        logging.info(\"Updated opponent\")\n",
    "        \n",
    "    # Train\n",
    "    logging.debug(\"Playing games\")\n",
    "    for episode in tqdm_notebook(range(EPISODES_PER_ITERATION), desc='Episode {}'.format(iteration), leave=False):\n",
    "        # Play two games, one for each black-white configuration\n",
    "        black_won, num_steps = rl_utils.play_a_game(replay_mem, go_env, black_policy=actor, white_policy=opponent, \n",
    "                                                    black_first=episode%2==0, max_steps=MAX_STEPS)\n",
    "        metrics['win_rate'].update_state(black_won)\n",
    "        metrics['num_steps'].update_state(num_steps)\n",
    "        \n",
    "    recent_ratings.append(metrics['win_rate'].result().numpy())\n",
    "        \n",
    "    # Update the models (also shuffles memory)\n",
    "    logging.debug(\"Updating model...\")\n",
    "    random.shuffle(replay_mem)\n",
    "    update_critic(replay_mem, iteration)\n",
    "    update_actor(replay_mem, iteration)  \n",
    "    \n",
    "    # Log results and resets the metrics\n",
    "    logging.debug(\"Logging metrics to tensorboard...\")\n",
    "    rl_utils.log_to_tensorboard(summary_writers['train'], metrics, iteration, replay_mem, actor, critic)\n",
    "    \n",
    "    # Reset memory\n",
    "    replay_mem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyMNxMAWwJR_"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate against different levels of opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_utils.evaluate(go_env, actor, opponent, LEVEL_PATHS, MAX_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play against our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97j_uYY9wJSA",
    "outputId": "4d6aa1e6-8b63-4a39-b600-e331284ad6ff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=7)\n",
    "rl_utils.play_against(actor, go_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "go_ai.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
