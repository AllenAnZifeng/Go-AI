{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88zLHqvDwJRj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from go_ai import data, metrics, mcts, models, policies\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfY-_1_5wJR5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ju4HYnKwJRq"
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DX04C6TbwJR5"
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 256\n",
    "EPISODES_PER_ITERATION = 128\n",
    "MAX_STEPS = 2 * BOARD_SIZE**2\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lECYa5ggGa7W"
   },
   "outputs": [],
   "source": [
    "NUM_EVAL_GAMES = 32\n",
    "ITERATIONS_PER_EVAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8pWCj8jGa7Y"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ix9zQ82Ga7a"
   },
   "outputs": [],
   "source": [
    "MC_SIMS = 0\n",
    "TEMP_FUNC = lambda step: (1/2) if (step < 16) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfJcqNeEGa7b"
   },
   "outputs": [],
   "source": [
    "WEIGHTS_DIR = 'model_weights/'\n",
    "ACTOR_CRITIC_PATH = WEIGHTS_DIR + 'checkpoint_{}x{}.h5'.format(BOARD_SIZE, BOARD_SIZE)\n",
    "LOAD_SAVED_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyALIMpwJRo"
   },
   "source": [
    "# Go Environment\n",
    "Train on a small board with heuristic reward for fast training and efficient debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4w7gMrfwJRp"
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbI1xPoVGa7e"
   },
   "source": [
    "# Metrics and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc1lqGjPwJRm"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuHFwINdGa7g"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4hPKfRaGa7g"
   },
   "outputs": [],
   "source": [
    "tb_metrics = {}\n",
    "for metric_key in ['val_loss', 'overall_loss', 'num_steps', 'move_loss']:\n",
    "    tb_metrics[metric_key] = tf.keras.metrics.Mean('{}'.format(metric_key), \n",
    "                                                   dtype=tf.float32)\n",
    "tb_metrics['pred_win_acc'] = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNKtplyZGa7h"
   },
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvHrRtTfGa7h"
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/actor_critic/{}/main'.format(current_time)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7ZoRfzIwJRr"
   },
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VDp7tLiGa7j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor_critic = models.make_actor_critic(BOARD_SIZE, 'val_net', 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x7SlLhZGa7l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tf.keras.utils.plot_model(actor_critic, to_file='logs/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "1FsLyPsAGa7m",
    "outputId": "6786c8ef-0958-4c82-d4ac-40cb4ceebb3a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_critic\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "board (InputLayer)              [(None, 7, 7, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 7, 7, 64)     3520        board[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 7, 7, 64)     256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 7, 7, 64)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     36928       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 7, 64)     256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 7, 7, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 2)      130         re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 1)      65          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 2)      8           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 1)      4           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 7, 7, 2)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 7, 7, 1)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 98)           0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 49)           0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           4950        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "invalid_values (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           2500        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 50)           0           dense[0][0]                      \n",
      "                                                                 invalid_values[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "valid_moves (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "move_probs (Softmax)            (None, 50)           0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            51          re_lu_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 48,668\n",
      "Trainable params: 48,406\n",
      "Non-trainable params: 262\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlpiIb4KGa7p"
   },
   "outputs": [],
   "source": [
    "opponent = tf.keras.models.clone_model(actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4m-C0k6Ga7q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved models\n"
     ]
    }
   ],
   "source": [
    "if LOAD_SAVED_MODELS:\n",
    "    actor_critic.load_weights(ACTOR_CRITIC_PATH)\n",
    "    opponent.load_weights(ACTOR_CRITIC_PATH)\n",
    "    print(\"Loaded saved models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = go_env.get_state()\n",
    "my_policy = policies.MctPolicy(actor_critic, state, MC_SIMS, TEMP_FUNC)\n",
    "opponent_policy = policies.MctPolicy(opponent, state, MC_SIMS, TEMP_FUNC)\n",
    "greedy_policy = policies.MctGreedyPolicy(state)\n",
    "random_policy = policies.RandomPolicy()\n",
    "human_policy = policies.HumanPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVCEKWx_Ga7r"
   },
   "source": [
    "# Demo Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSQ1RFHuGa7r"
   },
   "source": [
    "Symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98jmZoKvwJRv",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 s, sys: 228 ms, total: 2.63 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metrics.plot_symmetries(go_env, actor_critic, 'logs/symmetries.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqVpHyRLGa7s"
   },
   "source": [
    "Plot a whole game trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kkqnueRjGa7t",
    "outputId": "098a59a2-c1a0-4e9b-8344-cd44a8fc8b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.86 s, sys: 221 ms, total: 7.08 s\n",
      "Wall time: 6.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "traj, _ = data.self_play(go_env, policy=my_policy, max_steps=MAX_STEPS, \n",
    "                         get_symmetries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrTjrYjBGa7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 1.23 s, total: 18.2 s\n",
      "Wall time: 9.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fig = metrics.state_responses(actor_critic, traj)\n",
    "fig.savefig('logs/a_trajectory.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNBj_gKPwJR_"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-oRYYzzGa7v"
   },
   "outputs": [],
   "source": [
    "actor_critic_opt = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "replay_mem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LT1PUXyXwJR_",
    "outputId": "100dda95-316e-457c-b1f3-c8bf82243c52",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Self Play: 100%|██████████| 128/128 [30:09<00:00,  7.67s/it]  \n",
      "Updating:   0%|          | 0/2374 [00:00<?, ?it/s]WARNING: Logging before flag parsing goes to stderr.\n",
      "W0919 10:46:16.135141 4674909632 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Updating: 100%|██████████| 2374/2374 [02:01<00:00, 19.07it/s, 79.7% 0.529L]\n",
      "Evaluation: 100%|██████████| 8/8 [00:27<00:00,  3.48s/it, 1.0 100.0%]\n",
      "Evaluation: 100%|██████████| 8/8 [01:14<00:00,  9.64s/it, 1.0 87.5%]\n",
      "Evaluation: 100%|██████████| 32/32 [04:37<00:00,  9.07s/it, 0.0 31.2%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2%O, 100.0%R, 87.5%G Rejected new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Self Play: 100%|██████████| 128/128 [15:39<00:00,  7.05s/it]\n",
      "Updating: 100%|██████████| 2353/2353 [01:56<00:00, 20.26it/s, 81.6% 0.476L]\n",
      "Evaluation: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it, 1.0 100.0%]\n",
      "Evaluation: 100%|██████████| 8/8 [01:01<00:00,  7.34s/it, 1.0 100.0%]\n",
      "Evaluation: 100%|██████████| 32/32 [04:12<00:00,  8.11s/it, 1.0 43.8%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.8%O, 100.0%R, 100.0%G Rejected new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2 - Self Play: 100%|██████████| 128/128 [2:14:36<00:00,  7.15s/it]     \n",
      "Updating: 100%|██████████| 2356/2356 [02:09<00:00, 18.15it/s, 82.0% 0.469L]\n",
      "Evaluation: 100%|██████████| 8/8 [00:24<00:00,  2.92s/it, 1.0 100.0%]\n",
      "Evaluation: 100%|██████████| 8/8 [01:16<00:00,  9.48s/it, 0.0 50.0%]\n",
      "Evaluation: 100%|██████████| 32/32 [04:51<00:00, 10.04s/it, 0.0 37.5%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.5%O, 100.0%R, 50.0%G Rejected new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 3 - Self Play:  53%|█████▎    | 68/128 [08:33<07:35,  7.58s/it]"
     ]
    }
   ],
   "source": [
    "for iteration in range(ITERATIONS):\n",
    "    # Train\n",
    "    episode_pbar = tqdm(range(EPISODES_PER_ITERATION), \n",
    "                        desc='Iteration {} - Self Play'.format(iteration), \n",
    "                        leave=True, position=0)\n",
    "    for episode in episode_pbar:\n",
    "        trajectory, num_steps = data.self_play(go_env, policy=my_policy, \n",
    "                                               max_steps=MAX_STEPS)\n",
    "        replay_mem.extend(trajectory)\n",
    "        tb_metrics['num_steps'].update_state(num_steps)\n",
    "        \n",
    "    # Update the models (also shuffles memory)\n",
    "    random.shuffle(replay_mem)\n",
    "    np_data = data.replay_mem_to_numpy(replay_mem)\n",
    "    batched_np_data = [np.array_split(datum, len(replay_mem) // BATCH_SIZE) \n",
    "                       for datum in np_data]\n",
    "    batched_mem = list(zip(*batched_np_data))\n",
    "    models.update_win_prediction(actor_critic, batched_mem, actor_critic_opt, \n",
    "                                 iteration, tb_metrics)\n",
    "    \n",
    "    # Evaluate against previous model\n",
    "    if (iteration+1) % ITERATIONS_PER_EVAL == 0:\n",
    "        rand_win_rate = metrics.evaluate(go_env, my_policy, random_policy,\n",
    "                                                 max_steps=MAX_STEPS, \n",
    "                                                 num_games=8)\n",
    "        greed_win_rate = metrics.evaluate(go_env, my_policy, greedy_policy,\n",
    "                                                 max_steps=MAX_STEPS, \n",
    "                                                 num_games=8)\n",
    "        opp_win_rate = metrics.evaluate(go_env, my_policy, opponent_policy, \n",
    "                                    max_steps=MAX_STEPS, \n",
    "                                    num_games=NUM_EVAL_GAMES)\n",
    "        \n",
    "        stats = \"{:.1f}%O, {:.1f}%R, {:.1f}%G\".format(100*opp_win_rate, \n",
    "                                                     100*rand_win_rate,\n",
    "                                                     100*greed_win_rate)\n",
    "        if opp_win_rate > 0.6:\n",
    "            actor_critic.save_weights(ACTOR_CRITIC_PATH)\n",
    "            opponent.load_weights(ACTOR_CRITIC_PATH)\n",
    "            \n",
    "            print(\"{} Accepted new model\".format(stats))\n",
    "        else:\n",
    "            print(\"{} Rejected new model\".format(stats))\n",
    "            actor_critic.load_weights(ACTOR_CRITIC_PATH)\n",
    "    \n",
    "    # Log results and resets the metrics\n",
    "    \n",
    "    metrics.log_to_tensorboard(summary_writer, tb_metrics, iteration, go_env, \n",
    "                               actor_critic, TEMP_FUNC, 'logs/a_trajectory.jpg')\n",
    "    # Reset memory\n",
    "    replay_mem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyMNxMAWwJR_"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Moy9uJ6fGa7z"
   },
   "source": [
    "Play against our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97j_uYY9wJSA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)\n",
    "data.pit(go_env, black_policy=opponent_policy, white_policy=human_policy, max_steps=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emKESg3hGa71"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of go_ai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
