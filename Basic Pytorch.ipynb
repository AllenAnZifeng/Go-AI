{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "import numpy as np\n",
    "from go_ai import data\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "BOARD_SIZE = 5\n",
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(6 * board_size * board_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(256, board_size * board_size + 1),\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        invalid_values = data.batch_invalid_values(state)\n",
    "        x = torch.flatten(state, start_dim=1)\n",
    "        x = self.main(x)\n",
    "        policy = self.policy(x)\n",
    "        policy += invalid_values\n",
    "        policy = nn.functional.softmax(policy, dim=1)\n",
    "        value = self.value(x)\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0414, 0.0378, 0.0377, 0.0373, 0.0401, 0.0384, 0.0393, 0.0372, 0.0364,\n",
      "         0.0370, 0.0394, 0.0394, 0.0408, 0.0407, 0.0382, 0.0357, 0.0373, 0.0400,\n",
      "         0.0365, 0.0369, 0.0378, 0.0379, 0.0392, 0.0395, 0.0392, 0.0387]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4983]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PolicyValueNet(BOARD_SIZE)\n",
    "state = go_env.get_state()\n",
    "state_batches = state[np.newaxis]\n",
    "states_tensor = torch.from_numpy(state_batches).type(torch.FloatTensor)\n",
    "policy, value = net(states_tensor)\n",
    "print(policy)\n",
    "print(value)\n",
    "policy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, model1, model2):\n",
    "    states = []\n",
    "    \n",
    "    env.reset()\n",
    "    state = env.get_canonical_state()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        turn = go_env.turn()\n",
    "        state_tensor = torch.from_numpy(state[np.newaxis]).type(torch.FloatTensor)\n",
    "        if turn == 0:\n",
    "            action_probs, _ = model1(state_tensor)\n",
    "        else:\n",
    "            action_probs, _ = model2(state_tensor)\n",
    "        action = np.random.choice(np.arange(0, env.size * env.size + 1), p=action_probs.detach().numpy()[0])\n",
    "        _, _, done, _ = env.step(action)\n",
    "        state = env.get_canonical_state()\n",
    "        states.append(state)\n",
    "    winner = go_env.get_winner()\n",
    "    canonical_winners = [winner if i % 2 == 0 else 1 - winner for i in range(len(states))]\n",
    "    return states, canonical_winners\n",
    "\n",
    "def generate_trajectories(env, model1, model2, num_episodes):\n",
    "    state_list = []\n",
    "    winner_list = []\n",
    "    for i in range(num_episodes):\n",
    "        states, winners = play_game(env, model1, model2)\n",
    "        state_list.extend(states)\n",
    "        winner_list.extend(winners)\n",
    "    return state_list, winner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, winners = generate_trajectories(go_env, net, net, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(model, opt, states, winners, batch_size):\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    winner_batches = np.array_split(winners, len(winners) // batch_size)\n",
    "    pbar = tqdm_notebook(range(len(state_batches)))\n",
    "    for b in pbar:\n",
    "        b_s = torch.from_numpy(state_batches[b]).type(torch.FloatTensor)\n",
    "        b_w = winner_batches[b]\n",
    "        b_w_tensor = torch.from_numpy(b_w).type(torch.FloatTensor)\n",
    "        opt.zero_grad()\n",
    "        _, pred_win = model(b_s)\n",
    "        loss = nn.functional.binary_cross_entropy(pred_win, b_w_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        correct = (pred_win > 0.5).type(torch.IntTensor)[:,0] == b_w_tensor.type(torch.IntTensor)\n",
    "        accuracy = np.mean(correct.numpy())\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()) + ' Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15343da695b1421f86a793d5bc46df1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=114), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([33])) that is different to the input size (torch.Size([33, 1])) is deprecated. Please ensure they have the same size.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_eval(net, opt, states, winners, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qvals(env, model, states):\n",
    "    canonical_next_states = batch_canonical_children_states(states)\n",
    "    _, canonical_next_vals = model(canonical_next_states)\n",
    "\n",
    "    curr_idx = 0\n",
    "    batch_qvals = []\n",
    "    for state in states:\n",
    "        valid_moves = env.gogame.get_valid_moves(state)\n",
    "        Qs = []\n",
    "        for move in range(env_gogame.get_action_size(state)):\n",
    "            if valid_moves[move]:\n",
    "                canonical_next_state = canonical_next_states[curr_idx]\n",
    "                terminal = env.gogame.get_game_ended(canonical_next_state)\n",
    "                winning = canonical_winning(canonical_next_state)\n",
    "                oppo_val = (1 - terminal) * canonical_next_vals[curr_idx].item() + (terminal) * winning\n",
    "                qval = invert_qval(oppo_val)\n",
    "                Qs.append(qval)\n",
    "                curr_idx += 1\n",
    "            else:\n",
    "                Qs.append(0)\n",
    "\n",
    "        batch_qvals.append(Qs)\n",
    "\n",
    "    assert curr_idx == len(canonical_next_vals), (curr_idx, len(canonical_next_vals))\n",
    "    return np.array(batch_qvals)\n",
    "\n",
    "def policy_iter(env, model, opt, states, batch_size):\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    pbar = tqdm_notebook(state_batches)\n",
    "    for states in pbar:\n",
    "        states_tensor = torch.from_numpy(states).type(torch.FloatTensor)\n",
    "        policy, _ = model(states_tensor)\n",
    "        qvals = get_qvals(env, model, states)\n",
    "        greedy = np.argmax(qvals, axis=1)\n",
    "        opt.zero_grad()\n",
    "        loss = nn.functional.cross_entropy(policy, greedy)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066d88e1829741bc9e8b1d72490f9e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=114), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_canonical_children_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e87b945d3f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-74a92a8ac637>\u001b[0m in \u001b[0;36mpolicy_iter\u001b[0;34m(env, model, opt, states, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstates_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mqvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_qvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-74a92a8ac637>\u001b[0m in \u001b[0;36mget_qvals\u001b[0;34m(env, model, states)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_qvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcanonical_next_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_canonical_children_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonical_next_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonical_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurr_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_canonical_children_states' is not defined"
     ]
    }
   ],
   "source": [
    "policy_iter(go_env, net, opt, states, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
