{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "import numpy as np\n",
    "from go_ai import data\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "BOARD_SIZE = 5\n",
    "MODEL_SAVE_FILE = 'models/actorcritic_{0}x{0}.h5'.format(BOARD_SIZE)\n",
    "LOAD_TRAINED = True\n",
    "\n",
    "TEMP_DECAY = 0.9\n",
    "INIT_TEMP = 1\n",
    "MIN_TEMP = 1\n",
    "\n",
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self, board_size, init_temp, min_temp):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.temp = init_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.main1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 3, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main2 = nn.Sequential(\n",
    "            nn.Linear(board_size ** 2, board_size ** 2),\n",
    "            nn.BatchNorm1d(board_size ** 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(board_size ** 2, board_size ** 2 + 1),\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(board_size ** 2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        invalid_values = data.batch_invalid_values(state)\n",
    "        x = self.main1(state)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.main2(x)\n",
    "        policy = self.policy(x)\n",
    "        policy += invalid_values\n",
    "        policy /= self.temp\n",
    "        policy = nn.functional.softmax(policy, dim=1)\n",
    "        value = self.value(x)\n",
    "        return policy, value\n",
    "    \n",
    "    def decay_temp(self, decay):\n",
    "        self.temp *= decay\n",
    "        self.temp = max(self.temp, self.min_temp)\n",
    "        \n",
    "class RandomAgent():\n",
    "    def __call__(self, state):\n",
    "        valid_moves = go_env.gogame.get_valid_moves(state[0])\n",
    "        # Do not pass if possible\n",
    "        if np.sum(valid_moves) > 1:\n",
    "            valid_moves[-1] = 0\n",
    "        probs = valid_moves / np.sum(valid_moves)\n",
    "        return torch.from_numpy(probs[np.newaxis]), None\n",
    "    \n",
    "    def eval(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, model1, model2):\n",
    "    states = []\n",
    "    \n",
    "    env.reset()\n",
    "    state = env.get_canonical_state()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        turn = env.turn()\n",
    "        state_tensor = torch.from_numpy(state[np.newaxis]).type(torch.FloatTensor)\n",
    "        if turn == 0:\n",
    "            action_probs, _ = model1(state_tensor)\n",
    "        else:\n",
    "            action_probs, _ = model2(state_tensor)\n",
    "        action = np.random.choice(np.arange(0, env.size * env.size + 1), p=action_probs.detach().numpy()[0])\n",
    "        _, _, done, _ = env.step(action)\n",
    "        state = env.get_canonical_state()\n",
    "        states.append(state)\n",
    "    winner = env.get_winner()\n",
    "    canonical_winners = [winner if i % 2 == 0 else 1 - winner for i in range(len(states))]\n",
    "    return states, canonical_winners\n",
    "\n",
    "def generate_trajectories(env, model1, model2, num_episodes):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    state_list = []\n",
    "    winner_list = []\n",
    "    pbar = tqdm_notebook(range(num_episodes), desc='Trajectory generation')\n",
    "    for i in pbar:\n",
    "        states, winners = play_game(env, model1, model2)\n",
    "        state_list.extend(states)\n",
    "        winner_list.extend(winners)\n",
    "        pbar.set_postfix_str('Average length: ' + str(len(state_list) / (i + 1)))\n",
    "    return state_list, winner_list\n",
    "\n",
    "def pit(env, model1, model2, num_episodes):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model1_wins = 0\n",
    "    model2_wins = 0\n",
    "    pbar = tqdm_notebook(range(num_episodes // 2), desc='Playing black')\n",
    "    for i in pbar:\n",
    "        _, winners = play_game(env, model1, model2)\n",
    "        if winners[0] == 1:\n",
    "            model1_wins += 1\n",
    "        elif winners[0] == 0:\n",
    "            model2_wins += 1\n",
    "        pbar.set_postfix_str('Black WR: {}'.format(model1_wins / (i + 1)))\n",
    "    black_wins = model1_wins\n",
    "    pbar = tqdm_notebook(range(num_episodes // 2), desc='Playing white')\n",
    "    for i in pbar:\n",
    "        _, winners = play_game(env, model2, model1)\n",
    "        if winners[0] == 1:\n",
    "            model2_wins += 1\n",
    "        elif winners[0] == 0:\n",
    "            model1_wins += 1\n",
    "        pbar.set_postfix_str('White WR: {}'.format((model1_wins - black_wins) / (i + 1)))\n",
    "    print('Model 1 WR: {}'.format(model1_wins / num_episodes))\n",
    "    print('Model 2 WR: {}'.format(model2_wins / num_episodes))\n",
    "    return model1_wins, model2_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(model, opt, states, winners, batch_size):\n",
    "    model.train()\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    winner_batches = np.array_split(winners, len(winners) // batch_size)\n",
    "    total_correct = 0\n",
    "    pbar = tqdm_notebook(range(len(state_batches)), desc='Policy evaluation')\n",
    "    for b in pbar:\n",
    "        b_s = torch.from_numpy(state_batches[b]).type(torch.FloatTensor)\n",
    "        b_w = winner_batches[b]\n",
    "        b_w_tensor = torch.from_numpy(b_w).type(torch.FloatTensor)\n",
    "        opt.zero_grad()\n",
    "        _, pred_win = model(b_s)\n",
    "        loss = nn.functional.binary_cross_entropy(pred_win[:,0], b_w_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        correct = (pred_win > 0.5).type(torch.IntTensor)[:,0] == b_w_tensor.type(torch.IntTensor)\n",
    "        total_correct += np.sum(correct.numpy())\n",
    "        accuracy = total_correct / (batch_size * (b + 1))\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()) + ' Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from go_ai.montecarlo import invert_qval, canonical_winning, batch_canonical_children_states\n",
    "\n",
    "def get_qvals(env, model, states):\n",
    "    canonical_next_states = batch_canonical_children_states(states)\n",
    "    next_states_tensor = torch.from_numpy(canonical_next_states).type(torch.FloatTensor)\n",
    "    _, canonical_next_vals = model(next_states_tensor)\n",
    "\n",
    "    curr_idx = 0\n",
    "    batch_qvals = []\n",
    "    for state in states:\n",
    "        valid_moves = env.gogame.get_valid_moves(state)\n",
    "        Qs = []\n",
    "        for move in range(env.gogame.get_action_size(state)):\n",
    "            if valid_moves[move]:\n",
    "                canonical_next_state = canonical_next_states[curr_idx]\n",
    "                terminal = env.gogame.get_game_ended(canonical_next_state)\n",
    "                winning = canonical_winning(canonical_next_state)\n",
    "                oppo_val = (1 - terminal) * canonical_next_vals[curr_idx].item() + (terminal) * winning\n",
    "                qval = invert_qval(oppo_val)\n",
    "                Qs.append(qval)\n",
    "                curr_idx += 1\n",
    "            else:\n",
    "                Qs.append(0)\n",
    "\n",
    "        batch_qvals.append(Qs)\n",
    "\n",
    "    assert curr_idx == len(canonical_next_vals), (curr_idx, len(canonical_next_vals))\n",
    "    return np.array(batch_qvals)\n",
    "\n",
    "def policy_iter(env, model, opt, states, batch_size):\n",
    "    model.train()\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    pbar = tqdm_notebook(state_batches, desc='Policy iteration')\n",
    "    for states in pbar:\n",
    "        states_tensor = torch.from_numpy(states).type(torch.FloatTensor)\n",
    "        policy, _ = model(states_tensor)\n",
    "        qvals = get_qvals(env, model, states)\n",
    "        greedy = np.argmax(qvals, axis=1)\n",
    "        greedy_tensor = torch.from_numpy(greedy).type(torch.LongTensor)\n",
    "        opt.zero_grad()\n",
    "        loss = nn.functional.cross_entropy(policy, greedy_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(env, model, opt, batch_size, num_episodes):\n",
    "    states, winners = generate_trajectories(go_env, model, model, num_episodes)\n",
    "    policy_eval(model, opt, states, winners, batch_size)\n",
    "    policy_iter(env, model, opt, states, batch_size)\n",
    "    model.decay_temp(TEMP_DECAY)\n",
    "    \n",
    "def train(env, model, iterations, lr, batch_size, eps_per_iter):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(iterations):\n",
    "        print('Iteration {}'.format(i))\n",
    "        train_step(env, model, opt, batch_size, eps_per_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = PolicyValueNet(BOARD_SIZE, INIT_TEMP, MIN_TEMP)\n",
    "if LOAD_TRAINED:\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_FILE))\n",
    "    \n",
    "train(go_env, model, iterations=10, lr=0.001, batch_size=32, eps_per_iter=256)\n",
    "torch.save(model.state_dict(), MODEL_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4016d1cc1308450981049df802b745f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing black', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e894b18473bc44b8a3a24b5a336141f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing white', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1 WR: 0.518\n",
      "Model 2 WR: 0.328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(259, 164)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = PolicyValueNet(5, 1, 1)\n",
    "baseline.load_state_dict(torch.load('models/acbaseline_5x5.h5'))\n",
    "pit(go_env, model, baseline, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f7e4ced33a4c0687aa2167ba55274f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing black', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b150c8e2a644c7be542dd150ab11e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing white', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1 WR: 0.924\n",
      "Model 2 WR: 0.058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(462, 29)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = RandomAgent()\n",
    "pit(go_env, model, rand, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
